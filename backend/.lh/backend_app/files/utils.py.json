{
    "sourceFile": "backend_app/files/utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1752216713283,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1752216713283,
            "name": "Commit-0",
            "content": "import pandas as pd\r\nimport numpy as np\r\nfrom sklearn import preprocessing, model_selection, linear_model, neighbors, tree, ensemble, svm, metrics\r\nimport joblib\r\n\r\n\r\ndef read_file(file):\r\n    import os\r\n    ext = os.path.splitext(file.name)[1].lower()\r\n    file.seek(0)\r\n    if ext == '.csv':\r\n        return pd.read_csv(file)\r\n    elif ext == '.xlsx':\r\n        return pd.read_excel(file)\r\n    else:\r\n        raise ValueError(\"Unsupported file type\")\r\n\r\n\r\ndef preprocess_and_train(df, config):\r\n    # Create fresh copies to avoid modifying original data\r\n    df_processed = df.copy()\r\n    \r\n    # Initialize preprocessing objects\r\n    encoder = None\r\n    if config['encoder'] == 'LabelEncoder':\r\n        encoder = preprocessing.LabelEncoder()\r\n    elif config['encoder'] == 'OneHotEncoder':\r\n        encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False)\r\n\r\n    scaler = None\r\n    if config['scaler'] == 'StandardScaler':\r\n        scaler = preprocessing.StandardScaler()\r\n    elif config['scaler'] == 'MinMaxScaler':\r\n        scaler = preprocessing.MinMaxScaler()\r\n\r\n    # Handle missing values - critical for categorical targets\r\n    for col in df_processed.select_dtypes(exclude=object):\r\n        df_processed[col] = df_processed[col].fillna(df_processed[col].mean())\r\n\r\n    for col in df_processed.select_dtypes(include=object):\r\n        df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0])\r\n\r\n    numerical_cols = df_processed.select_dtypes(exclude=object).columns\r\n    df_processed = safe_remove_outliers(df_processed, numerical_cols)\r\n\r\n    # Split features and target - ensure no missing values in target\r\n    X = df_processed.loc[:,config['features']]\r\n    y = df_processed[config['target']]\r\n\r\n        #Encoding\r\n    for i in X.select_dtypes(include=object):\r\n        X[i] = encoder.fit_transform(X[i])\r\n        \r\n    # Scaling\r\n    X_scaled = scaler.fit_transform(X)\r\n\r\n    if len(X) != len(y):\r\n        print(f'Pre-split mismatch: X has {len(X)} samples, y has {len(y)}')\r\n\r\n    # Train-test split with consistent indices\r\n    if config['stratify']:\r\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\r\n            X_scaled, y,\r\n            test_size=config['test_size'],\r\n            random_state=config['random_state'],\r\n            stratify=y\r\n        )\r\n    else:\r\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\r\n            X_scaled, y,\r\n            test_size=config['test_size'],\r\n            random_state=config['random_state']\r\n        )\r\n\r\n    model_type = config['model_type']\r\n\r\n    parameters = config['parameters'] if len(config['parameters'])>0 else None\r\n\r\n    model_map = {\r\n        'LinearRegression': linear_model.LinearRegression,\r\n        'LogisticRegression': linear_model.LogisticRegression,\r\n        'KNeighborsRegressor': neighbors.KNeighborsRegressor,\r\n        'KNeighborsClassifier': neighbors.KNeighborsClassifier,\r\n        'DecisionTreeRegressor': tree.DecisionTreeRegressor,\r\n        'DecisionTreeClassifier': tree.DecisionTreeClassifier,\r\n        'RandomForestRegressor': ensemble.RandomForestRegressor,\r\n        'RandomForestClassifier': ensemble.RandomForestClassifier,\r\n        'SVC': svm.SVC,\r\n        'Ridge': linear_model.Ridge\r\n    }\r\n\r\n    # Model training\r\n    model = model_map[model_type](**parameters)\r\n\r\n    model.fit(X_train, y_train)\r\n    \r\n    # Evaluation\r\n    prediction = model.predict(X_test)\r\n    accuracy = evaluate_model(y_test, prediction, config['problem_type'], config['features'], model)\r\n    \r\n    return model, encoder, scaler, accuracy\r\n\r\n\r\ndef evaluate_model(y_test, prediction, problem_type, features, model):\r\n    accuracy = {}\r\n    \r\n    if problem_type == 'classification':\r\n        accuracy['accuracy_score'] = metrics.accuracy_score(y_test, prediction)\r\n        accuracy['precision'] = metrics.precision_score(y_test, prediction, average='weighted')\r\n        accuracy['recall'] = metrics.recall_score(y_test, prediction, average='weighted')\r\n        accuracy['f1_score'] = metrics.f1_score(y_test, prediction, average='weighted')\r\n        \r\n        # Confusion matrix with labels\r\n        unique_classes = sorted(np.unique(y_test))\r\n        accuracy['confusion_matrix'] = {\r\n            'matrix': metrics.confusion_matrix(y_test, prediction).tolist(),\r\n            'labels': [str(cls) for cls in unique_classes]\r\n        }\r\n        accuracy['classification_report'] = metrics.classification_report(\r\n            y_test, prediction, output_dict=True)\r\n    else:\r\n        accuracy['r2_score'] = metrics.r2_score(y_test, prediction)\r\n        accuracy['mean_squared_error'] = metrics.mean_squared_error(y_test, prediction)\r\n        accuracy['mean_absolute_error'] = metrics.mean_absolute_error(y_test, prediction)\r\n        accuracy['root_mean_squared_error'] = np.sqrt(metrics.mean_squared_error(y_test, prediction))\r\n\r\n    # Feature importance\r\n    feature_importance = {\r\n        'labels': features,\r\n        'values': [0]*len(features)  # Default to zeros\r\n    }\r\n\r\n    try:\r\n        if hasattr(model, 'feature_importances_'):\r\n            feature_importance['values'] = model.feature_importances_.tolist()\r\n        elif hasattr(model, 'coef_'):\r\n            feature_importance['values'] = np.abs(model.coef_).tolist()\r\n    except Exception as e:\r\n        print(f\"Couldn't get feature importance: {str(e)}\")\r\n\r\n    accuracy['feature_importance'] = feature_importance\r\n    \r\n    return accuracy\r\n\r\n\r\n# Remove outliers while maintaining alignment\r\ndef safe_remove_outliers(df, cols):\r\n    for col in cols:\r\n        q1 = df[col].quantile(0.25)\r\n        q3 = df[col].quantile(0.75)\r\n        iqr = q3 - q1\r\n        lower_bound = q1 - (1.5 * iqr)\r\n        upper_bound = q3 + (1.5 * iqr)\r\n        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\r\n    return df\r\n    \r\n\r\ndef load_model_and_predict(model_path, features, columns, encoder_path=None, scaler_path=None):\r\n    try:\r\n        print(\"\\n=== PREDICTION DEBUG START ===\")\r\n        print(f\"Input features: {features}\")\r\n        print(f\"Expected columns: {columns}\")\r\n        \r\n        # Load model components\r\n        model = joblib.load(model_path)\r\n        print(\"Model loaded successfully\")\r\n        \r\n        # Load preprocessing objects if they exist\r\n        scaler = joblib.load(scaler_path) if scaler_path else None\r\n        encoder = joblib.load(encoder_path) if encoder_path else None\r\n        \r\n        # Create DataFrame with features in correct order\r\n        input_df = pd.DataFrame([features], columns=columns)\r\n        print(\"\\nRaw input DataFrame:\")\r\n        print(input_df)\r\n        \r\n        # Apply preprocessing EXACTLY as done during training\r\n        processed_df = input_df.copy()\r\n        \r\n        # 1. Handle missing values (same as training)\r\n        for col in processed_df.select_dtypes(include=['float64', 'int64']):\r\n            processed_df[col] = processed_df[col].fillna(processed_df[col].mean())\r\n        \r\n        for col in processed_df.select_dtypes(include=['object']):\r\n            processed_df[col] = processed_df[col].fillna(processed_df[col].mode()[0])\r\n        \r\n        # 2. Apply encoding (if encoder exists)\r\n        if encoder is not None:\r\n            print(\"\\nBefore encoding:\")\r\n            print(processed_df.select_dtypes(include=['object']).columns)\r\n            \r\n            for col in processed_df.select_dtypes(include=['object']):\r\n                if col in columns:  # Only encode columns that were encoded during training\r\n                    processed_df[col] = encoder.transform(processed_df[col])\r\n            \r\n            print(\"After encoding:\")\r\n            print(processed_df.head())\r\n        \r\n        # 3. Apply scaling (if scaler exists)\r\n        if scaler is not None:\r\n            print(\"\\nBefore scaling:\")\r\n            print(processed_df.select_dtypes(include=['float64', 'int64']).head())\r\n            \r\n            numeric_cols = processed_df.select_dtypes(include=['float64', 'int64']).columns\r\n            processed_df[numeric_cols] = scaler.transform(processed_df[numeric_cols])\r\n            \r\n            print(\"After scaling:\")\r\n            print(processed_df.head())\r\n        \r\n        # Convert to numpy array for prediction\r\n        final_features = processed_df.values\r\n        print(\"\\nFinal features for prediction:\")\r\n        print(final_features)\r\n        \r\n        # Make prediction\r\n        prediction = model.predict(final_features)\r\n        print(f\"\\nRaw prediction: {prediction}\")\r\n        \r\n        print(\"=== PREDICTION DEBUG END ===\")\r\n        return prediction\r\n        \r\n    except Exception as e:\r\n        print(f\"\\n=== PREDICTION ERROR ===\")\r\n        print(f\"Error during prediction: {str(e)}\")\r\n        print(\"=== PREDICTION ERROR ===\")\r\n        raise ValueError(f\"Prediction failed: {str(e)}\")"
        }
    ]
}